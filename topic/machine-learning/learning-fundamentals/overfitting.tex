\documentclass[../../../topic_machine-learning]{subfiles}

\begin{document}

\sectionline
\section{過学習}

訓練データではうまくいっているのに、訓練時には見なかった未知のデータではうまくいかない状態を\keywordJE{過学習}{overfitting}という。

\subsection{過学習が起こる原理}

たくさんのデータを集めれば、仮説が本当に成立するかどうかを高い確率で検証することができる。

\br

データ数に対して仮説数が多い場合、正しい仮説よりも、たまたま成り立ってしまう誤った仮説が含まれる可能性が高くなる。
これが過学習が起こる場合である。

\begin{itemize}
  \item 検証する仮説数が少ない場合：事例をすべて満たしている仮説が本当に成り立つ可能性が高い
  \item 検証する仮説数が多い場合：事例をすべて満たしている仮説も、たまたま成り立っているだけの可能性が高い
\end{itemize}

機械学習は、多くの仮説（パラメータがある値をとる時のモデル）の中から、多くの訓練事例を説明する仮説がどれかを探す問題ともいえる。

このとき、過学習は、たまたま多くの訓練事例でうまくいくようなモデルが見つかってしまうことで起こる。

\subsection{過学習を防ぐ原理}

訓練データ数に比べて、検証する仮説数が少ない状況であれば、見つかった仮説がたまたま成り立ったものでなく、実際に関係がある可能性が高くなる。

\br

そのため、過学習を防止するには、
\begin{equation*}
  \text{\bfseries 訓練データ数} > \text{\bfseries 検証する仮説数}
\end{equation*}
という状況を目指すことが有効となる。

\subsection{過学習の防止策：訓練データを増やす}

訓練データを増やすことができれば、多くの事例で仮説を検証できるので、たまたま仮説が成り立つ可能性を小さくすることができる。

\br

しかし、訓練データを増やすことは困難である場合も多い。

そこで、訓練データに意味を変えないような変換を加えて、人工的にデータを水増しする\keywordJE{データオーグメンテーション}{data augmentation}という手法が有効となる。

\subsection{過学習の防止策：仮説数を少なくする}

訓練データ数が同じであれば、その中で仮説数を少なくすることも過学習の防止として有効である。

\begin{theorem}{オッカムの剃刀}
  ある事柄を説明するためには、必要以上に多くを仮定すべきでない
\end{theorem}

仮説数を少なくするには、単純にはモデルのパラメータ数を少なくすればよい。

モデルに制約を加えることで、可能な限り単純なモデルを使うことで、仮説数を少なくすることができる。

\br

しかし、過学習を抑える別の仕組みがある場合は、必ずしもパラメータ数が少ない方が汎化するとは限らない。

たとえば、\keywordJE{ニューラルネットワーク}{neural network}は、パラメータ数が膨大であるにも関わらず、高い汎化能力を持っている。

\end{document}
